{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as data\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import json\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CocoDataset(data.Dataset):\n",
    "    \"\"\"COCO Custom Dataset compatible with torch.utils.data.DataLoader.\"\"\"\n",
    "\n",
    "    def __init__(self, root, origin_file, split, img_tags, vocab):\n",
    "        \"\"\"Set the path for images, captions and vocabulary wrapper.\n",
    "\n",
    "        Args:\n",
    "            root: image directory.\n",
    "            json: coco annotation file path.\n",
    "            vocab: vocabulary wrapper.\n",
    "            transform: image transformer.\n",
    "        \"\"\"\n",
    "        self.root = root\n",
    "        if split in {'train', 'restval'}:\n",
    "            self.split = ['train', 'restval']\n",
    "        if split in {'val'}:\n",
    "            self.split = ['val']\n",
    "        if split in {'test'}:\n",
    "            self.split = ['test']\n",
    "        \n",
    "        with open(origin_file, 'r') as j:\n",
    "            self.origin_file = json.load(j)\n",
    "        \n",
    "        self.images_id = [self.origin_file['images'][index]['imgid'] \\\n",
    "                     for index in range(0,len(self.origin_file['images'])) \\\n",
    "                     if self.origin_file['images'][index]['split'] in self.split]\n",
    "        \n",
    "        with open(img_tags, 'r') as j:\n",
    "            self.img_tags = json.load(j)\n",
    "      \n",
    "        with open(vocab, 'r') as j:\n",
    "            self.vocab = json.load(j)\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.RandomCrop(224),\\\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.485, 0.456, 0.406),\n",
    "                                 (0.229, 0.224, 0.225))])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Returns one data pair (image and caption).\"\"\"\n",
    "        \n",
    "        word2id = self.vocab['word_map']\n",
    "        ID = self.images_id[index]\n",
    "        \n",
    "        img_id = self.origin_file['images'][ID]['imgid']\n",
    "        path = self.origin_file['images'][ID]['filepath'] + \\\n",
    "            '/'+self.origin_file['images'][ID]['filename']\n",
    "\n",
    "        image = Image.open(os.path.join(self.root, path)).convert('RGB')\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Convert caption (string) to word ids.\n",
    "        tags = []\n",
    "        t = list(map(str.lower, self.img_tags[str(ID)]))\n",
    "        tags = [word2id[token] for token in t]\n",
    "        target = torch.Tensor(tags)\n",
    "        return image, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images_id)\n",
    "\n",
    "\n",
    "def collate_fn(data):\n",
    "    \"\"\"Creates mini-batch tensors from the list of tuples (image, caption).\n",
    "\n",
    "    We should build custom collate_fn rather than using default collate_fn, \n",
    "    because merging caption (including padding) is not supported in default.\n",
    "    Args:\n",
    "        data: list of tuple (image, caption). \n",
    "            - image: torch tensor of shape (3, 256, 256).\n",
    "            - caption: torch tensor of shape (?); variable length.\n",
    "    Returns:\n",
    "        images: torch tensor of shape (batch_size, 3, 256, 256).\n",
    "        targets: torch tensor of shape (batch_size, padded_length).\n",
    "        lengths: list; valid length for each padded caption.\n",
    "    \"\"\"\n",
    "    # Sort a data list by caption length (descending order).\n",
    "    data.sort(key=lambda x: len(x[1]), reverse=True)\n",
    "    images, captions = zip(*data)\n",
    "\n",
    "    # Merge images (from tuple of 3D tensor to 4D tensor).\n",
    "    images = torch.stack(images, 0)\n",
    "\n",
    "    # Merge captions (from tuple of 1D tensor to 2D tensor).\n",
    "    lengths = [len(cap) for cap in captions]\n",
    "    targets = torch.zeros(len(captions), max(lengths)).long()\n",
    "    for i, cap in enumerate(captions):\n",
    "        end = lengths[i]\n",
    "        targets[i, :end] = cap[:end]\n",
    "    return images, targets, lengths\n",
    "\n",
    "\n",
    "def get_loader(root, origin_file, split,img_tags, vocab, batch_size, shuffle, num_workers):\n",
    "    \"\"\"Returns torch.utils.data.DataLoader for custom coco dataset.\"\"\"\n",
    "    # COCO caption dataset\n",
    "    coco = CocoDataset(root=root,\n",
    "                       origin_file=origin_file,\n",
    "                       split=split,\n",
    "                       img_tags=img_tags,\n",
    "                       vocab=vocab)\n",
    "\n",
    "    # Data loader for COCO dataset\n",
    "    # This will return (images, captions, lengths) for each iteration.\n",
    "    # images: a tensor of shape (batch_size, 3, 224, 224).\n",
    "    # captions: a tensor of shape (batch_size, padded_length).\n",
    "    # lengths: a list indicating valid length for each caption. length is (batch_size).\n",
    "    data_loader = torch.utils.data.DataLoader(dataset=coco,\n",
    "                                              batch_size=batch_size,\n",
    "                                              shuffle=shuffle,\n",
    "                                              num_workers=num_workers,\n",
    "                                              collate_fn=collate_fn)\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '/home/lkk/datasets/coco2014'\n",
    "origin_file = root+'/'+'dataset_coco.json'\n",
    "img_tags='./img_tags.json'\n",
    "voc = './vocab.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d=get_loader(root,origin_file,'train',img_tags,voc,8,True,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 3, 224, 224])\n",
      "torch.Size([8, 22])\n",
      "[22, 20, 18, 17, 16, 16, 11, 9]\n",
      "torch.Size([8, 3, 224, 224])\n",
      "torch.Size([8, 24])\n",
      "[24, 19, 17, 16, 16, 13, 12, 9]\n",
      "torch.Size([8, 3, 224, 224])\n",
      "torch.Size([8, 20])\n",
      "[20, 17, 16, 16, 15, 13, 11, 11]\n"
     ]
    }
   ],
   "source": [
    "for i,(a,b,c) in enumerate(d):\n",
    "    print(a.shape)\n",
    "    test=b\n",
    "    print(b.shape)\n",
    "    print(c)\n",
    "    if i == 2:break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[106, 268, 431,   1,  28, 242, 943, 216, 133, 114, 334, 590, 101,  74,\n",
       "          56, 198, 150, 505, 240,  12],\n",
       "        [933, 105,  80, 493,  27, 556, 666,  36,  53,  84,  19,  90,  61,  76,\n",
       "         201, 967,  17,   0,   0,   0],\n",
       "        [435,   1,  58,  73,  24, 759,  35,  18,  77, 540, 585,  70, 144,   6,\n",
       "         689,  75,   0,   0,   0,   0],\n",
       "        [110, 406,   1,  28,   3,  24,  34, 295, 444, 213,   5, 523, 156,   7,\n",
       "          47,  12,   0,   0,   0,   0],\n",
       "        [126,  48, 482, 405,   9,  13,   4, 120,  60, 182,  33,   6,  81, 555,\n",
       "           2,   0,   0,   0,   0,   0],\n",
       "        [865, 292,  86,  97, 124,  14, 190,  54,  22,  95,  79, 935,  55,   0,\n",
       "           0,   0,   0,   0,   0,   0],\n",
       "        [ 97, 108,  10, 111, 259, 408, 139, 167,  21,  17,  12,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0],\n",
       "        [ 40, 841, 449,  13, 118, 170, 325, 461, 479,   6, 388,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14161"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
