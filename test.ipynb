{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as data\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import json\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CocoDataset(data.Dataset):\n",
    "    \"\"\"COCO Custom Dataset compatible with torch.utils.data.DataLoader.\"\"\"\n",
    "\n",
    "    def __init__(self, root, origin_file, split, img_tags, vocab):\n",
    "        \"\"\"Set the path for images, captions and vocabulary wrapper.\n",
    "\n",
    "        Args:\n",
    "            root: image directory.\n",
    "            json: coco annotation file path.\n",
    "            vocab: vocabulary wrapper.\n",
    "            transform: image transformer.\n",
    "        \"\"\"\n",
    "        self.root = root\n",
    "        if split in {'train', 'restval'}:\n",
    "            self.split = ['train', 'restval']\n",
    "        if split in {'val'}:\n",
    "            self.split = ['val']\n",
    "        if split in {'test'}:\n",
    "            self.split = ['test']\n",
    "        \n",
    "        with open(origin_file, 'r') as j:\n",
    "            self.origin_file = json.load(j)\n",
    "        \n",
    "        self.images_id = [self.origin_file['images'][index]['imgid'] \\\n",
    "                     for index in range(0,len(self.origin_file['images'])) \\\n",
    "                     if self.origin_file['images'][index]['split'] in self.split]\n",
    "        \n",
    "        with open(img_tags, 'r') as j:\n",
    "            self.img_tags = json.load(j)\n",
    "      \n",
    "        with open(vocab, 'r') as j:\n",
    "            self.vocab = json.load(j)\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.RandomCrop(224),\\\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.485, 0.456, 0.406),\n",
    "                                 (0.229, 0.224, 0.225))])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Returns one data pair (image and caption).\"\"\"\n",
    "        \n",
    "        word2id = self.vocab['word_map']\n",
    "        ID = self.images_id[index]\n",
    "        \n",
    "        img_id = self.origin_file['images'][ID]['imgid']\n",
    "        path = self.origin_file['images'][ID]['filepath'] + \\\n",
    "            '/'+self.origin_file['images'][ID]['filename']\n",
    "\n",
    "        image = Image.open(os.path.join(self.root, path)).convert('RGB')\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Convert caption (string) to word ids.\n",
    "        tags = []\n",
    "        t = list(map(str.lower, self.img_tags[str(ID)]))\n",
    "        tags = [word2id[token] for token in t]\n",
    "        target = torch.Tensor(tags)\n",
    "        return image, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images_id)\n",
    "\n",
    "\n",
    "def collate_fn(data):\n",
    "    \"\"\"Creates mini-batch tensors from the list of tuples (image, caption).\n",
    "\n",
    "    We should build custom collate_fn rather than using default collate_fn, \n",
    "    because merging caption (including padding) is not supported in default.\n",
    "    Args:\n",
    "        data: list of tuple (image, caption). \n",
    "            - image: torch tensor of shape (3, 256, 256).\n",
    "            - caption: torch tensor of shape (?); variable length.\n",
    "    Returns:\n",
    "        images: torch tensor of shape (batch_size, 3, 256, 256).\n",
    "        targets: torch tensor of shape (batch_size, padded_length).\n",
    "        lengths: list; valid length for each padded caption.\n",
    "    \"\"\"\n",
    "    # Sort a data list by caption length (descending order).\n",
    "    data.sort(key=lambda x: len(x[1]), reverse=True)\n",
    "    images, captions = zip(*data)\n",
    "\n",
    "    # Merge images (from tuple of 3D tensor to 4D tensor).\n",
    "    images = torch.stack(images, 0)\n",
    "\n",
    "    # Merge captions (from tuple of 1D tensor to 2D tensor).\n",
    "    lengths = [len(cap) for cap in captions]\n",
    "    targets = torch.zeros(len(captions), max(lengths)).long()\n",
    "    for i, cap in enumerate(captions):\n",
    "        end = lengths[i]\n",
    "        targets[i, :end] = cap[:end]\n",
    "    return images, targets, lengths\n",
    "\n",
    "\n",
    "def get_loader(root, origin_file, split,img_tags, vocab, batch_size, shuffle, num_workers):\n",
    "    \"\"\"Returns torch.utils.data.DataLoader for custom coco dataset.\"\"\"\n",
    "    # COCO caption dataset\n",
    "    coco = CocoDataset(root=root,\n",
    "                       origin_file=origin_file,\n",
    "                       split=split,\n",
    "                       img_tags=img_tags,\n",
    "                       vocab=vocab)\n",
    "\n",
    "    # Data loader for COCO dataset\n",
    "    # This will return (images, captions, lengths) for each iteration.\n",
    "    # images: a tensor of shape (batch_size, 3, 224, 224).\n",
    "    # captions: a tensor of shape (batch_size, padded_length).\n",
    "    # lengths: a list indicating valid length for each caption. length is (batch_size).\n",
    "    data_loader = torch.utils.data.DataLoader(dataset=coco,\n",
    "                                              batch_size=batch_size,\n",
    "                                              shuffle=shuffle,\n",
    "                                              num_workers=num_workers,\n",
    "                                              collate_fn=collate_fn)\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '/home/lkk/datasets/coco2014'\n",
    "origin_file = root+'/'+'dataset_coco.json'\n",
    "img_tags='./img_tags.json'\n",
    "voc = './vocab.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d=get_loader(root,origin_file,'train',img_tags,voc,8,True,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,d in enumerate(d):\n",
    "#     print(a.shape)\n",
    "#     print(b.shape)\n",
    "    dd=d\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 3, 224, 224])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[147, 766,  62, 508,  32, 126, 235,  30, 104,  58, 129,  24,  10,   5,\n",
       "          23, 299,  42,  35, 467, 360, 514, 516,   6,  75,   1, 285],\n",
       "        [791,  19, 263,  38,  95, 239, 120,  32, 248,  87,  53,   3, 129,  17,\n",
       "         111,   4,  18, 393, 193, 165, 653, 119, 557, 135,  70,  55],\n",
       "        [391, 370,   9,  15,   3, 127, 312,  96,  23,   2, 222,  36, 611, 246,\n",
       "          13, 182, 407, 187, 245, 476,  94,   0,   0,   0,   0,   0],\n",
       "        [  2, 437,  53,   9,  19,  10,   6, 312,  44, 155, 822, 786,  15, 553,\n",
       "          26, 169,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [ 11,  89, 180, 721,   3,   4,  18, 195, 133, 134,  39, 139,  47,  45,\n",
       "         179,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [ 11, 273, 957,   1,  24, 356,   4, 367, 277, 659, 101, 272,  22,   6,\n",
       "          47,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [712,  87, 625,  43, 476, 100,   3, 127,  34, 658, 673, 173, 197, 201,\n",
       "         480,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [ 51, 369, 109, 352, 213, 330, 141, 921, 245, 711,  59,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_id = [file['images'][index]['imgid'] \\\n",
    "            for index in range(len(file['images'])) \\\n",
    "            if file['images'][index]['split'] in ['train', 'restval']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "113287"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(images_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(img_tags, 'r') as j:\n",
    "    img_tagss = json.load(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123287"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(img_tagss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys=list(img_tagss.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(img_tagss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ID in keys:\n",
    "    if str(ID) not in images_id:\n",
    "        img_tagss.pop(str(ID))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
